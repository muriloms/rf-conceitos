# Conceitos B√°sico de Reinforcement Learning
O *Reinforcement Learning* (RL) - Aprendizado por Refor√ßo - √© o problema de estudar um agente em um ambiente, onde
o agente precisa interagir com o ambiente para maximizar algumas recompensas cumulativas, aprendendo dentro de um ciclo de tentativa e erro
as melhores a√ß√µes.

![img-rf](https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/image3-5f8cbb1fb6fb9132fef76b13b8687bfc.png)

Exemplo: um agente em um labirinto tentando encontrar sua sa√≠da. Quanto mais r√°pido ele encontrar a sa√≠da, melhor ser√° a recompensa.

## Markov Decision Process (MDP)
MDP √© uma cole√ß√£o de estados, a√ß√µes, probabilidades de transi√ß√£o, recompensas e fator de desconto: (***S***, ***A***, ***P***, ***R***, ***Œ≥***)
- ***S*** √© um conjunto de um estado finito que descreve o ambiente
- ***A*** √© um conjunto de a√ß√µes finitas que descreve a a√ß√£o que pode ser executada pelo agente
- ***P*** √© uma matriz de probabilidade que indica a probabilidade de se mover de um estado para outro
- ***R*** √© um conjunto de recompensas que dependem do estado e das medidas tomadas. 
As recompensas n√£o s√£o necessariamente positivas, elas devem ser vistas como resultado de uma a√ß√£o realizada pelo agente quando ela est√° em um determinado estado. 
Portanto, recompensa negativa indica mau resultado, enquanto recompensa positiva indica bom resultado
- ***Œ≥*** √© um fator de desconto, que indica a import√¢ncia das recompensas futuras para o estado atual. O fator de desconto √© um valor entre 0 e 1. 
Uma recompensa R que ocorrer N etapas no futuro a partir do estado atual √© multiplicada por Œ≥ ^ N para descrever sua import√¢ncia para o estado atual. Por exemplo, considere Œ≥ = 0,9 e uma recompensa R = 10 que est√° 3 passos √† frente do nosso estado atual. 
A import√¢ncia dessa recompensa para n√≥s de onde estamos √© igual a (0,9¬≥) * 10 = 7,29.

# Fun√ß√µes de valor
Como o agente deve agir nesse ambiente?
A regra que impomos ao agente √© que ele deve agir de maneira a maximizar as recompensas coletadas.
Para poder fazer isso, o agente deve ter algo para estimar a posi√ß√£o ou estado em que est√° atualmente. Considere novamente o labirinto: se o agente estiver a poucos passos da sa√≠da, sua posi√ß√£o ter√° muito mais valor do que uma posi√ß√£o na centro do labirinto.
Chamamos essa estimativa de Valor da posi√ß√£o ou Valor do estado. Uma vez que conhecemos o valor de cada estado, podemos descobrir qual √© a melhor maneira de agir (simplesmente seguindo o estado com o valor mais alto).
Ainda precisamos descobrir uma maneira de quantificar o valor de cada estado. Isso √© feito por uma fun√ß√£o chamada Sate-Value Function:

![img-svfunction](https://miro.medium.com/max/531/1*NNCFu-9jq25GKTaDXdUzjQ.png)
[mais informa√ß√µes sobre a equa√ß√£o](https://towardsdatascience.com/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4)

O valor de cada estado √© a m√©dia das recompensas futuras com desconto,
***v(s)*** √© expresso em termos de recompensas imediatas ***r*** valor dos estados vizinhos ***v(s')***

### Interpreta√ß√µes da equa√ß√£o
- quando esta num estado ***s***, considere todas as a√ß√µes poss√≠veis;
- para cada a√ß√£o ***a*** obtenha a probabilidade ***p(s', r | s, a)*** de receber recompensa imediata ***r*** e mudar para o estado vizinho ***s'***, 
sabendo que estamos no estado ***s*** e realizamos a a√ß√£o ***a***;
- adicione a recompensa ***r*** ao valor descontado do estado vizinho ***s'*** dado por ***Œ≥ * v (s')***. 
Multiplique o resultado por ***p (s ', r | s, a)*** e obtemos ***p (s', r | s, a) * [r + Œ≥ * v (s ')]***;
- Repita as etapas 2 e 3 para todos os estados ***s'*** vizinhos ***s***, bem como todas as recompensas poss√≠veis ***r*** e calcule a soma dos resultados. 
Agora temos ***Sum (p (s ', r | s, a) * [r + Œ≥ * v (s')])*** sobre todos os ***s'*** e ***r***;
- Isso fornecer√° a m√©dia das recompensas futuras com desconto ao executar apenas uma a√ß√£o ***a***. 
No entanto, existem v√°rias a√ß√µes a serem tomadas, portanto, precisamos calcular a m√©dia de todas as a√ß√µes. 
Para fazer isso, multiplicamos pela probabilidade ***ùúã (a | s)*** de que a a√ß√£o ***a*** seja executada no estado ***s***, 
e somamos todas as a√ß√µes poss√≠veis nesse estado.

*(quando estamos em um estado, temos uma probabilidade de tomar alguma a√ß√£o que pode nos levar a diferentes estados com diferentes recompensas. 
Portanto, o valor do nosso estado √© a m√©dia de todas as recompensas com descontos que poder√≠amos receber ao executar todas as a√ß√µes poss√≠veis no estado atual.)*

 C√°lculo para obter um n√∫mero que represente o valor do atual estado.
 
 ***v(s)*** diz como √© bom estar no estado ***s***, mas n√£o diz como √© bom executar a a√ß√£o ***a*** por um tempo no estado ***s***. 
 Este √© o objetivo da fun√ß√£o Action-Value:
 
 ![img-avfunction](https://miro.medium.com/max/344/1*wYxr2uTHTqtkTQ5wneDWsg.png)
 
 Interpreta√ß√£o: tomamos ***v(s)*** e, em vez de executar todas as a√ß√µes, decidimos executar apenas uma a√ß√£o. 
 N√£o nos perguntamos mais qual √© a probabilidade de usar a a√ß√£o sobre todas as a√ß√µes poss√≠veis, mas dizemos que usaremos a a√ß√£o ***a***. 
 Portanto, a soma de ***ùúã (a | s)*** n√£o se aplica mais. Com essa l√≥gica, reduzimos ***v(s)*** da m√©dia de todas as a√ß√µes poss√≠veis para simplesmente 
 usar uma a√ß√£o selecionada, denotamos isso como ***q (s, a)***.
 
 Observe que ***q(s, a)*** verifica o valor da a√ß√£o no estado ***s*** enquanto os valores ***v(s')*** dos estados vizinhos s√£o mantidos inalterados. 
 Portanto, nesse sentido, verifica como essa a√ß√£o √© boa no estado atual, mantendo todo o resto igual.


Tamb√©m √© f√°cil perceber que ***v(s)*** √© a m√©dia ponderada de ***q(s, a)*** em todas as a√ß√µes poss√≠veis no estado ***s***.

![img](https://miro.medium.com/max/229/1*LyWMoIu2JtC5GaAqIS9T3A.png)
